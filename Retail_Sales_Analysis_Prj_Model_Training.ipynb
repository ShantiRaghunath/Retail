{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baa288e2-5f9e-428f-98c8-ebd980c7c6a5",
   "metadata": {},
   "source": [
    "# Package Installation for Snowflake Connection and Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b943968-da70-42a3-8b10-18ac34471919",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install  dask[complete]  snowflake  snowflake-connector-python snowflake-snowpark-python snowflake-snowpark-python[pandas] seaborn matplotlib numpy pandas scikit-learn  fosforml plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273cb1f1-515b-4aaf-a866-271c35f9d532",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fea648-ed49-4a20-8881-f30c51a95a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing general libraries\n",
    "import pandas as pd  # For data manipulation\n",
    "import numpy as np  # For numerical operations\n",
    "import seaborn as sns  # For data visualization\n",
    "from datetime import datetime  # For date and time manipulation\n",
    "import matplotlib.pyplot as plt  # For plotting graphs\n",
    "\n",
    "# Importing Snowflake session management\n",
    "from snowflake.snowpark.session import Session  # For Snowflake integration\n",
    "\n",
    "# Importing advanced plotting libraries\n",
    "from plotly.subplots import make_subplots  # For creating subplots in Plotly\n",
    "import plotly.graph_objects as go  # For creating interactive visualizations with Plotly\n",
    "import plotly.express as px  # For simplified plotting using Plotly\n",
    "\n",
    "# Setting Pandas display option to show more columns in the output\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "# Importing libraries for machine learning and feature selection\n",
    "from sklearn.ensemble import RandomForestClassifier  # For Random Forest model\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, precision_recall_curve, auc  # For model evaluation metrics\n",
    "from sklearn.feature_selection import SelectKBest, f_classif  # For feature selection\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder  # For encoding categorical data and scaling\n",
    "from sklearn.impute import SimpleImputer  # For handling missing values\n",
    "from sklearn.compose import ColumnTransformer  # For applying different preprocessing steps to different columns\n",
    "from sklearn.pipeline import Pipeline  # For creating machine learning pipelines\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV  # For splitting data and performing grid search for hyperparameter tuning\n",
    "\n",
    "# Importing additional libraries for visualization and matplotlib handling\n",
    "import matplotlib.pyplot as plt  # For plotting static visualizations\n",
    "\n",
    "# Setting up environment for plotting using Seaborn and Matplotlib\n",
    "sns.set(style=\"whitegrid\")  # Setting the style for Seaborn plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bee066-c521-4a3a-80b2-0aac915578f2",
   "metadata": {},
   "source": [
    "# This section connects to Snowflake using fosforml's Snowflake session manager, retrieves data from a specified Snowflake table, and loads the data into a Pandas DataFrame for further processing and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eca5766-1741-4495-b7d1-2814de5f21bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing the get_session function from fosforml's Snowflake session manager\n",
    "from fosforml.model_manager.snowflakesession import get_session\n",
    "\n",
    "# Establishing a Snowflake session for executing queries and performing operations\n",
    "my_session = get_session()\n",
    "\n",
    "# Define the name of the Snowflake table to query\n",
    "table_name = 'ORDER_DATA_TRAINING'\n",
    "\n",
    "# Execute a SQL query to select all records from the specified table in Snowflake\n",
    "df_sample = my_session.sql(\"select * from {}\".format(table_name)).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e7fc9a-ae74-43f1-848e-c07264951961",
   "metadata": {},
   "source": [
    "# Filtering and Preparing the Training Dataset for Returned Status Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9999fc71-b900-4bad-8d6d-a0754c6ff0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dask for handling larger datasets efficiently\n",
    "import dask.dataframe as dd\n",
    "\n",
    "#  Dask DataFrame allows for efficient handling of large datasets and delayed execution\n",
    "df_sample_dask = dd.from_pandas(df_sample, npartitions=4)  # Adjust npartitions based on data size and memory\n",
    "\n",
    "#  We use Dask to filter the records based on the 'RETURNED_STATUS' column\n",
    "df_train_dask = df_sample_dask[df_sample_dask['RETURNED_STATUS'].isin(['CANCELLED', 'RETURNED', 'DELIVERED', 'IN PROCESS'])]\n",
    "\n",
    "#  Dask performs lazy computation, so we need to explicitly call .compute() to trigger the actual computation\n",
    "df_train = df_train_dask.compute()\n",
    "\n",
    "#  Display the count of each 'RETURNED_STATUS' to verify the filtering step\n",
    "print(df_train['RETURNED_STATUS'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5fce19-5e2f-4a60-996a-1d14f1bc3da8",
   "metadata": {},
   "source": [
    "# Creating a Binary Target Variable for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382df3c0-7920-45fb-ab71-7999d3d0fb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the 'RETURNED_STATUS' is 'CANCELLED' or 'RETURNED', assign 1, else assign 0\n",
    "df_train['TARGET'] = df_train['RETURNED_STATUS'].apply(lambda x: 1 if x in ['CANCELLED', 'RETURNED'] else 0)\n",
    "\n",
    "\n",
    "# This will give us an understanding of how many records are marked as 1 (CANCELLED/RETURNED) or 0 (DELIVERED/IN PROCESS)\n",
    "target_counts = df_train['TARGET'].value_counts()\n",
    "\n",
    "# Display the count of each target class\n",
    "print(target_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b382928-904e-42b6-afae-3c1f0c4200e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0dbb87-548a-4f38-be71-70e26a7d6cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['SHIPPING_DELAY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c04f679-a353-45da-b7ab-ef87e7fcab44",
   "metadata": {},
   "source": [
    "# Preparing Features and Target for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34000b0-f03a-4ab4-9bd1-142f27af0686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'TARGET' column has already been created where 'CANCELLED' and 'RETURNED' are assigned 1, and the rest are 0\n",
    "# Drop 'RETURNED_STATUS' and 'TARGET' from the feature set (X_train) as they are not input features for the model\n",
    "X_train = df_train.drop(columns=['RETURNED_STATUS', 'TARGET'])\n",
    "\n",
    "# The target variable (y_train) is the 'TARGET' column we created earlier\n",
    "y_train = df_train['TARGET']\n",
    "\n",
    "# Identify the numerical and categorical columns for separate preprocessing\n",
    "numerical_cols = ['SHIPMENT_SLA','UNIT_PRICE', 'WM_PICKING_AGE', 'WM_PACKING_AGE', 'WM_ORDER_AGE', 'STORE_ID',]  # Numerical features\n",
    "categorical_cols = [ 'DIVISION_CODE', 'DIVISION_NAME', 'BRAND_CODE', 'BRAND_NAME', 'CLASS_CODE', 'CLASS_NAME', 'SELLING_CHANNEL', 'CHAIN', 'WEB_ORDER_NUMBER', 'OMS_LINE_ITEM_ID', 'OMS_TICKET_ID', 'SKU_ID', 'CURRENT_STATUS', 'CURRENT_STATUS_DESCRIPTION', 'SHIP_FROM_WAREHOUSE_DESCRIPTION',  'CARRIER_NAME', 'CARRIER_TRACKING_NUMBER', 'DROPSHIP_FLAG', 'ORDER_STATUS', 'WM_ORDER_ID', 'WM_ORDER_LINE_ID', 'WM_ORDER_STATUS', 'STORE_NAME', 'SHIP_METHOD_CODE', 'SHIP_METHOD_NAME', 'SHIP_METHOD_SERVICE',  'RETURN_REASON', 'RETURN_FLAG']  # Categorical features\n",
    "\n",
    "# Display the list of numerical and categorical columns for verification\n",
    "print(\"Numerical Columns:\", numerical_cols)\n",
    "print(\"\\n\\nCategorical Columns:\", categorical_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a58030-eceb-41d7-8de5-adee1adeaa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[categorical_cols] = X_train[categorical_cols].fillna(X_train[categorical_cols].mode().iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36216383-6e28-40cd-a147-8a47433c74b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[categorical_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02086ab0-924d-481e-9c27-0f64ed0f0425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count NaN values in each column\n",
    "df_sample[categorical_cols].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8733505-3978-4788-83d0-7f918a0bd8da",
   "metadata": {},
   "source": [
    "# Preprocessing Data and Building a Machine Learning Pipeline with Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5778a6e-2002-4878-9945-65abef184a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Define a pipeline for processing numerical columns\n",
    "numerical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Impute missing numerical values with the mean\n",
    "    ('scaler', StandardScaler())  # Scale the numerical features to have zero mean and unit variance\n",
    "])\n",
    "\n",
    "#  Define a pipeline for processing categorical columns\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing categorical values with the most frequent value\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))  # One-hot encode categorical features, ignoring unknown categories\n",
    "])\n",
    "\n",
    "#  Combine both pipelines into a ColumnTransformer\n",
    "# The ColumnTransformer applies the numerical and categorical transformations to their respective columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, numerical_cols),  # Apply numerical pipeline to numerical columns\n",
    "        ('cat', categorical_pipeline, categorical_cols)  # Apply categorical pipeline to categorical columns\n",
    "    ]\n",
    ")\n",
    "\n",
    "# The full pipeline consists of the preprocessing step followed by the RandomForest classifier\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),  # First, apply preprocessing\n",
    "    ('classifier', RandomForestClassifier(random_state=42, class_weight='balanced'))  # Then, train the RandomForest model\n",
    "])\n",
    "\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9202715d-80a5-4bfc-9ceb-ca1ee948a874",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in numerical_cols:\n",
    "    mean_value = pd.to_numeric(X_train[col], errors='coerce').mean()  \n",
    "    X_train[col].fillna(mean_value, inplace=True)\n",
    "\n",
    "\n",
    "for col in categorical_cols:\n",
    "    most_frequent = X_train[col].mode(dropna=True)[0] \n",
    "    X_train[col].fillna(most_frequent, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb7b510-dcd4-4bbd-8e35-9a54b7586948",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[categorical_cols].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d92383-d730-465c-ad64-9b85533d8871",
   "metadata": {},
   "outputs": [],
   "source": [
    " X_train[col].mode(dropna=True), col\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c5d2e6-c9a2-47fd-9a90-530a7bea81f4",
   "metadata": {},
   "source": [
    "#  Hyperparameter Tuning with GridSearchCV for Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b0d756-a66b-4f2c-bd76-95dbbac9760b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "# This grid contains possible values for several hyperparameters of the Random Forest model.\n",
    "param_dist = {\n",
    "    'classifier__n_estimators': [100, 200, 300],  # Number of trees in the forest\n",
    "    'classifier__max_depth': [10, 20, None],  # Maximum depth of the trees\n",
    "    'classifier__min_samples_split': [2, 5, 10],  # Minimum samples required to split a node\n",
    "    'classifier__min_samples_leaf': [1, 2, 4],  # Minimum samples required at each leaf node\n",
    "    'classifier__max_features': ['sqrt', 'log2'],  # Number of features to consider when looking for the best split\n",
    "    'classifier__max_samples': [0.5, 1.0]  # Proportion of samples to use for fitting, to avoid overfitting\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV will randomly sample combinations of hyperparameters, and evaluate them.\n",
    "random_search = RandomizedSearchCV(pipeline, param_distributions=param_dist, \n",
    "                                   n_iter=10, cv=3, n_jobs=-1, verbose=2, \n",
    "                                   scoring='f1_weighted', random_state=42)\n",
    "\n",
    "# This split is necessary to evaluate model performance on unseen data (the test set).\n",
    "X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the model on the training data using RandomizedSearchCV to find the optimal hyperparameters.\n",
    "random_search.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Step 11: Display the best hyperparameters found by RandomizedSearchCV\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b98d57b-6c39-49ab-b3cb-32c5f68b9a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_split.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a8c3be-7268-401e-b31d-deb62854fe44",
   "metadata": {},
   "source": [
    "#  Evaluating the Best Model on the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb07bc01-2a41-4d27-bc0d-ba66ee8faa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best_pipeline contains the model with the best hyperparameters found by GridSearchCV\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "\n",
    "#  Predict the labels (TARGET) for the test data and evaluate model performance\n",
    "y_pred_split = best_pipeline.predict(X_test_split)\n",
    "\n",
    "#  The classification report provides key metrics (precision, recall, F1-score) for both classes (CANCELLED/RETURNED and DELIVERED/IN PROCESS)\n",
    "print(\"Classification Report on CANCELLED/RETURNED records:\")\n",
    "print(classification_report(y_test_split, y_pred_split))\n",
    "\n",
    "#  Display the confusion matrix for further evaluation\n",
    "conf_matrix = confusion_matrix(y_test_split, y_pred_split)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bc8e2c-67a3-4760-bec6-5a798f31837b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072e6c0c-6e6b-4484-b718-cc1e7af39297",
   "metadata": {},
   "source": [
    "#  Making Predictions on DELIVERED and IN PROCESS Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93e5171-8d95-41e6-bdef-ed1b44807ab3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We only want to predict whether DELIVERED or IN PROCESS orders will be CANCELLED or RETURNED\n",
    "df_predict = df_sample[df_sample['RETURNED_STATUS'].isin(['DELIVERED', 'IN PROCESS'])]\n",
    "\n",
    "#  Drop the 'RETURNED_STATUS' column, as it's not needed for making predictions\n",
    "X_predict = df_predict.drop(columns=['RETURNED_STATUS'])\n",
    "\n",
    "#  Use the best pipeline (with preprocessing and the trained model) to predict on the filtered dataset\n",
    "predictions = best_pipeline.predict(X_predict)\n",
    "\n",
    "#  The 'PREDICTED_CANCELLED_RETURNED' column contains the predicted labels (1 for CANCELLED/RETURNED, 0 for not)\n",
    "df_predict['PREDICTED_CANCELLED_RETURNED'] = predictions\n",
    "\n",
    "#  Display the original 'RETURNED_STATUS' along with the new 'PREDICTED_CANCELLED_RETURNED' column\n",
    "df_predict[['RETURNED_STATUS', 'PREDICTED_CANCELLED_RETURNED']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30712700-3019-4e27-a290-3f4085773609",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_predict[['RETURNED_STATUS', 'PREDICTED_CANCELLED_RETURNED']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677444c5-dc09-4357-bb2f-dcc1a3be77b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Get the best model\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "\n",
    "# Step 12: Evaluate the best model on the test split\n",
    "y_pred_split = best_pipeline.predict(X_test_split)\n",
    "print(\"Classification Report on CANCELLED/RETURNED/DELIVERED/IN PROCESS records:\")\n",
    "print(classification_report(y_test_split, y_pred_split))\n",
    "\n",
    "# Step 13: Plot Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test_split, y_pred_split)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# Step 14: Classification Report (Precision, Recall, F1-Score) as a bar plot\n",
    "report = classification_report(y_test_split, y_pred_split, output_dict=True)\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "report_df[['precision', 'recall', 'f1-score']].iloc[:-3].plot(kind='bar')\n",
    "plt.title('Precision, Recall, F1-Score by Class')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=0)\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n",
    "\n",
    "# Step 15: ROC Curve and AUC (for binary classification tasks)\n",
    "if len(best_pipeline.classes_) == 2:  # Check if binary classification\n",
    "    # Predict probabilities for the positive class\n",
    "    y_prob = best_pipeline.predict_proba(X_test_split)[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_split, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "# Step 16: Precision-Recall Curve\n",
    "precision, recall, thresholds_pr = precision_recall_curve(y_test_split, y_prob)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, label=f'AUC = {auc(recall, precision):.2f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0131f8b-1128-4556-a05b-8709bb11e82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PREDICTED_CANCELLED_RETURNED'] =  grid_search.best_estimator_.predict(df[df_sample.columns.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e146e1a-a626-46e5-94a2-127eaf84239c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['RETURNED_STATUS', 'PREDICTED_CANCELLED_RETURNED']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f87ac0-a218-4259-8aec-881fa5ba88df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('PREDICTED_RETURN_STATUS', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2467b12f-73e7-4e2a-9052-4417942e1f31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e48e0a8-577d-4017-bccc-1401278e0a5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Importing the get_session function from fosforml's Snowflake session manager\n",
    "from fosforml.model_manager.snowflakesession import get_session\n",
    "\n",
    "# Establishing a Snowflake session for executing queries and performing operations\n",
    "my_session = get_session()\n",
    "\n",
    "# Define the name of the Snowflake table to query\n",
    "table_name = 'ORDER_DATA_3009'\n",
    "\n",
    "# Execute a SQL query to select all records from the specified table in Snowflake\n",
    "df_sample = my_session.sql(\"select * from {}\".format(table_name)).to_pandas()\n",
    "\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149949ed-b477-48fc-a748-8c43e2903e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample[categorical_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e96653-67de-4f89-9fa9-ac6b32bba975",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.drop('PREDICTED_CANCELLED_RETURNED', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eb646d-45b6-4c9c-9367-f2c987579564",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea015ce-3514-469e-8432-a596abd0d25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Pandas DataFrame (cust_df) into a Snowflake DataFrame\n",
    "training_datadf = my_session.createDataFrame(df_sample)\n",
    "\n",
    "# Write the Snowflake DataFrame to a Snowflake table named 'casino_customers'\n",
    "# The 'overwrite' mode ensures that the table is replaced if it already exists\n",
    "training_datadf.write.mode(\"overwrite\").save_as_table(\"ORDER_DATA_FINAL\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
